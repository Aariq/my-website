<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Eric R. Scott</title>
    <link>http://ericrscott.com/post/</link>
    <description>Recent content in Posts on Eric R. Scott</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Eric R. Scott</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>RStudio::conf reflections</title>
      <link>http://ericrscott.com/2019/01/23/rstudio-conf-reflections/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2019/01/23/rstudio-conf-reflections/</guid>
      <description>&lt;p&gt;This was my first time attending &lt;a href=&#34;https://www.rstudio.com/conference/&#34;&gt;RStudio::conf&lt;/a&gt;, and I went primarily to explore my career options in data science. I mainly stuck to teaching and modeling related talks since that’s how I already use R. Here are my major takeaways from the conference.&lt;/p&gt;
&lt;div id=&#34;shiny-is-the-new-hotness&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Shiny is the new hotness&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; apps are interactive web apps that run on R code, and there was a big focus on Shiny development at the conference this year. Almost everyone I talked to was using Shiny in their jobs including creating &lt;a href=&#34;https://shiny.rstudio.com/gallery/bus-dashboard.html&#34;&gt;dashboards&lt;/a&gt;, &lt;a href=&#34;https://laderast.github.io/burro/&#34;&gt;interactive exploratory data analysis&lt;/a&gt;, guiding industry researchers through statistical analyses, and &lt;a href=&#34;https://kbodwin.shinyapps.io/Lab_Exercise_tDist/&#34;&gt;teaching focused apps&lt;/a&gt; built on the &lt;code&gt;learnr&lt;/code&gt; package. There was also a lot of focus on scaling Shiny apps so many users could access apps simultaneously without significant slowdown.&lt;/p&gt;
&lt;p&gt;I’ve been toying with the idea of creating a Shiny app to help with my own work in doing some data quality checks on GC/MS data, and this gave me the inspiration to commit to doing it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rstudio-cloud-makes-teaching-r-painless&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;RStudio Cloud makes teaching R painless&lt;/h1&gt;
&lt;p&gt;I’ve taught an &lt;a href=&#34;https://github.com/Aariq/biotstatistics-recitation-2018&#34;&gt;intro to R for Biostatistics&lt;/a&gt; course twice now, and both times the first day of class feels like 80% fixing package installation errors. &lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt; allows students to access RStudio through a web interface, without downloading or installing anything. It also lets instructors set up project spaces with all the necessary packages &lt;strong&gt;already installed&lt;/strong&gt;. This allows you to start the first day off with fun stuff, like data visualization, and save the lessons about CRAN and troubleshooting package installations for later. Not only can you set up environments for students to work in, you can also peek into their environments. That means no more “I can maybe help you if you send me your code” emails!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;when-to-make-a-cran-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;When to make a CRAN package&lt;/h1&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;quot;I wish I&amp;#39;d left this code across scattered .R files instead of combining it into a package&amp;quot; said no one ever &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;http://t.co/udeNH4T67H&#34;&gt;http://t.co/udeNH4T67H&lt;/a&gt;&lt;/p&gt;&amp;mdash; David Robinson (@drob) &lt;a href=&#34;https://twitter.com/drob/status/611885584584441856?ref_src=twsrc%5Etfw&#34;&gt;June 19, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;I had already taken this advice and built a package for myself with all the functions that I’ve written and used in multiple projects. I called it &lt;code&gt;chemhelper&lt;/code&gt; and put it up on &lt;a href=&#34;https://github.com/Aariq/chemhelper&#34;&gt;GitHub&lt;/a&gt;, just in case someone else would find it useful. Now I’m working on a manuscript that uses some of the functions in this package, and I needed advice on what to do to make my analysis reproducible and archivable upon submitting it. You see, &lt;code&gt;webchem&lt;/code&gt; is very much a work in progress, so if I were to archive analysis code that relied on it, it would likely be broken very quickly and therefore not reproducible. One option is submitting my package to CRAN and then recording version information in the analysis code or using something like &lt;a href=&#34;https://rstudio.github.io/packrat/&#34;&gt;packrat&lt;/a&gt;. The advice I got over and over was &lt;strong&gt;if your package is potentially useful to people other than you, put it on CRAN&lt;/strong&gt;. I’ve already started pulling out the functions that are useful to others and plan on submitting a package to CRAN before submitting my manuscript!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-we-teach-coding&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How we teach coding&lt;/h1&gt;
&lt;p&gt;Finally, the keynote by &lt;a href=&#34;https://twitter.com/felienne&#34;&gt;@felienne&lt;/a&gt; was &lt;strong&gt;phenomenal&lt;/strong&gt;! You should watch it regardless of your area of interest—it’s &lt;em&gt;that&lt;/em&gt; kind of talk. With hand-drawn slides and an incredible stage presence, Dr. Felienne Hermans explored the weirdness of how we teach programming. For example, you wouldn’t just hand someone a guitar and say “the best way to learn is to just try changing something and see what happens!” and you also wouldn’t tell a child riding a bike with training wheels “that’s not real biking!”, but we do both of these things regularly when interacting with beginner programmers.&lt;/p&gt;
&lt;p&gt;Most importantly, we know empirically that reading out loud (phonics) is a good way to learn languages, and that &lt;em&gt;should include&lt;/em&gt; programming languages. I realized that part of the value in &lt;a href=&#34;http://varianceexplained.org/r/teach-tidyverse/&#34;&gt;teaching tidyverse first&lt;/a&gt; is that you can and &lt;em&gt;should&lt;/em&gt; read tidyverse code out-loud. I’m definitely going to make classrooms read code outloud in the future.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
iris %&amp;gt;%
  group_by(Species) %&amp;gt;% 
  summarize(mean_petal_length = mean(Petal.Length),
            sd_petal_length = sd(Petal.Length))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Say it with me, class:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Take the iris dataset, then&lt;br /&gt;
group it by Species, then&lt;br /&gt;
summarize it by taking the mean and standard deviation of petal length&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;What was your biggest takeaway from RStudio::conf 2019? Let me know in the comments!&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finding Cryptic Insect Eggs With Fluorescence</title>
      <link>http://ericrscott.com/2018/12/12/leafhopper-egg-method/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2018/12/12/leafhopper-egg-method/</guid>
      <description>&lt;p&gt;I recently gave a talk on some of my work as a PhD student on &lt;a href=&#34;http://www.ericrscott.com/project/climate-leafhopper-quality/&#34;&gt;experiments&lt;/a&gt; manipulating densities of the tea green leafhopper (&lt;em&gt;Empoasca onukii&lt;/em&gt;) on tea plants. What the audience liked most, I think, were my methods for finding leafhopper eggs in the field and rearing them in the lab (well, a guest room at a tea farm). You see, leafhoppers (including at least the tea green leafhopper and the small green leafhopper, &lt;em&gt;Empoasca vitis&lt;/em&gt;) lay their eggs &lt;em&gt;inside&lt;/em&gt; plant tissues, making them impossible to find with the naked eye. But, you can take advantage of the fluorescent properties of leafhopper eggs and plants to make them more visible.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/learning-SLEDA.jpeg&#34; alt=&#34;Students from TRI CAAS looking for leafhopper eggs in tea shoots&#34; width=&#34;70%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Students from TRI CAAS looking for leafhopper eggs in tea shoots
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It turns out that under blue light, leafhopper eggs fluoresce bright green while the chlorophyll in plants fluoresces red. By using blue light to create this fluorescence and blocking the blue light coming to your eyes with a filter, you can easily spot the eggs even though they are under the epidermis of the tea stems.&lt;/p&gt;
&lt;video autoplay=&#34;TRUE&#34; controls=&#34;TRUE&#34; loop=&#34;TRUE&#34; muted=&#34;TRUE&#34; src=&#34;http://ericrscott.com/img/SLEDA.mp4&#34; width=&#34;400&#34; height=&#34;400&#34; figcaption=&#34;testing&#34;&gt;&lt;/video&gt;
&lt;p&gt;What you’re seeing above is video from a smart phone with the lens covered by a pair of orange goggles. I move the beam of a blue LED flashlight (which you can’t really see because of the orange filter) toward the center of the frame, where you see a green dot appear on the tea stem. This is a leafhopper egg. Then I remove the orange goggles so you can see what it looks like without them—totally washed out by blue light.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/SLEDA-in-lab.png&#34; alt=&#34;A tea shoot lit by ambient light (left) and by blue light, filtered through orange goggles (right).  The leafhopper egg is the small green dot on the stem in the right panel.&#34; width=&#34;70%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: A tea shoot lit by ambient light (left) and by blue light, filtered through orange goggles (right). The leafhopper egg is the small green dot on the stem in the right panel.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This method is modified from a &lt;a href=&#34;http://link.springer.com/10.1007/BF03356146&#34;&gt;2004 paper by Herrmann and Böll&lt;/a&gt; that was written before bright blue LEDs were widely available, and calls for the use of full-spectrum white light from a halogen bulb passed through a filter. Now, blue LED flashlights are cheap and widely available, providing a much more portable solution for field sampling. I used a &lt;a href=&#34;https://www.amazon.com/WAYLLSHINE-Flashlight-Adjustable-Camping-Activities/dp/B00WD0XFAW&#34;&gt;small blue flashlight&lt;/a&gt; I bought from Amazon for my light source, and some &lt;a href=&#34;https://www.amazon.com/Uvex-S0360X-Ultra-spec-SCT-Orange-Anti-Fog/dp/B003OBZ64M/ref=sr_1_3?ie=UTF8&amp;amp;qid=1544637112&amp;amp;sr=8-3&amp;amp;keywords=uvex+orange+goggles&#34;&gt;orange, UV-blocking goggles&lt;/a&gt; for my blue light filter, but &lt;strong&gt;any&lt;/strong&gt; blue light source and orange goggles should work! I should also mention, that this is best done at dusk or nighttime to improve visibility.&lt;/p&gt;
&lt;p&gt;Whether you’re working in a vineyard to monitor &lt;em&gt;Empoasca vitis&lt;/em&gt;, trying to rear leafhoppers from eggs, or trying to count eggs in an oviposition choice experiment, using this method will save you time and frustration.&lt;/p&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acknowledgements:&lt;/h3&gt;
&lt;p&gt;Guan-Hua Liu and Long Jiao helped to confirm that the eggs I was finding were &lt;em&gt;Empoasca onukii&lt;/em&gt; eggs. Thanks guys!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Last Fieldwork Season in China</title>
      <link>http://ericrscott.com/2018/07/20/last-fieldwork-season/</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2018/07/20/last-fieldwork-season/</guid>
      <description>&lt;p&gt;I’m currently in Hangzhou, China at the &lt;a href=&#34;www.tricaas.com&#34;&gt;Tea Research Institute&lt;/a&gt;(TRI) for my fourth and last time. It’s bitter sweet (like my favorite teas ;-) ) since I’m both glad to be nearing the end of my PhD, and sad to say goodbye to all the friends I’ve made and a city I’ve really grown to enjoy living in.&lt;/p&gt;
&lt;div id=&#34;fieldwork&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fieldwork&lt;/h2&gt;
&lt;p&gt;This final summer, I’ve been focusing on a few experiments having to do with leafhoppers and their effects on tea chemistry (see the &lt;a href=&#34;http://ericrscott.com/project/climate-leafhopper-quality/&#34;&gt;project page&lt;/a&gt; for more info). I’m repeating an experiment from last year, where I put different densities of insects on tea plants and measure the chemistry, but this time using two different cultivars to see if they respond differently to insect attack.&lt;/p&gt;
I’m also trying my hand at making oolong tea!
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/myoolong.jpg&#34; alt=&#34;Jin Guan Yin leaves after two rounds of gentle bruising&#34; width=&#34;60%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Jin Guan Yin leaves after two rounds of gentle bruising
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is part of an experiment aiming to better understand when leafhoppers are a pest, and when they can improve tea quality. They’re definitely considered a pest to farmers that make green tea, but prized by some farmers making &lt;a href=&#34;https://specialtyteaalliance.org/world-of-tea/oriental-beauty-bug-bitten-teas/&#34;&gt;Eastern Beauty oolong&lt;/a&gt;. There are &lt;em&gt;many&lt;/em&gt; reasons for this, but I’m curious about how tea leaves react during processing after being previously damaged by leafhoppers. I’ve written a bit about this on &lt;a href=&#34;http://www.teageek.net/blog/2018/03/oolong-mid-oxidized/&#34;&gt;TeaGeek&lt;/a&gt; before, so you can read more about what’s going on during oolong processing there.&lt;/p&gt;
&lt;p&gt;I’m doing these two experiments at TRI’s new-ish experimental field station in near-by Shengzhou.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/shengzhou.jpg&#34; alt=&#34;A view of the Shengzhou TRI Exeprimental Station&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: A view of the Shengzhou TRI Exeprimental Station
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It is rather boring there since there are hardly any other students working there, it’s in a rural area, and it has barely functioning internet. But the weather last week there was &lt;em&gt;amazing&lt;/em&gt;. It was dry! It was actually dry! In China! I’ve never been anywhere &lt;strong&gt;not&lt;/strong&gt; extremely humid in China. In fact, it felt like home (California). Hot, dry, cool breezes at night, scattered fluffy clouds—perfect. I’ve always been lucky in that whenever I’m doing really grueling, repetitive, sweaty fieldwork, it’s at least been somewhere beautiful. That really keeps me going.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;people&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;People&lt;/h2&gt;
&lt;p&gt;I’m so happy to see old friends and meet new ones! I’ve been working closely with Dr. Li Xin and his masters student, Wei Ji Peng for the past 3 years and it’s great to see them again. I’ve also made friends in other departments (it’s easy to make friends at a tea research institute when you’re a foreign tea geek!) and it’s great to see them again as well.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/lixin-weijipeng.jpg&#34; alt=&#34;Wei Ji-Peng (left), me (center), and Dr. Li Xin (right) at the Shengzhou experimental station&#34; width=&#34;60%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Wei Ji-Peng (left), me (center), and Dr. Li Xin (right) at the Shengzhou experimental station
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I’ve also been lucky to have a Chinese REU student working with me this summer. Her name is Guo Ming Ming, and she’s been extremely helpful in carrying out experiments, catching leafhoppers, and translating (no, even after 4 years I do not speak Mandarin well enough to do much of anything on my own).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/ming-ming.jpg&#34; alt=&#34;Guo Ming-Ming, an undergraduate researcher visiting from Guangzhou for the summer&#34; width=&#34;60%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Guo Ming-Ming, an undergraduate researcher visiting from Guangzhou for the summer
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;computer-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Computer work&lt;/h2&gt;
&lt;p&gt;Fieldwork is fun, but its far too hot to work outside for most of the day. So I usually finish my morning work before breakfast most days, and occasionally do some work in the evening. Of course there are longer days for setting up experiments, but most of the time I’m in an office. I’ve been working a lot on writing and coding for a paper about using partial least squares regression (PLS) on ecological data. I’m learning a lot in the process, and I can’t wait to share it with everyone (it might even include &lt;a href=&#34;http://ericrscott.com/2018/03/05/cupcakes-vs-muffins/&#34;&gt;cupcakes vs. muffins&lt;/a&gt;!)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Retrieve chemical retention indices from NIST with {webchem}!</title>
      <link>http://ericrscott.com/2018/06/28/webchem/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2018/06/28/webchem/</guid>
      <description>&lt;p&gt;My PhD has involved learning a lot more than I expected about analytical chemistry, and as I’ve been learning, I’ve been trying my best to make my life easier by writing R functions to help me out. Some of those functions have found a loving home in the &lt;code&gt;webchem&lt;/code&gt; package, part of &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Papers that use gas chromatography to separate and measure chemicals often include a table of the compounds they found along with experimental retention indices and literature retention indices. A retention index is basically a corrected retention time—the time it took for the particular chemical to make it through the gas chromatograph, an instrument designed to separate chemicals, to the detector used to identify the compound (e.g. an FID or mass spectrometer). While the retention time for a particular compound might vary from run to run or beetween labs, the retention &lt;strong&gt;index&lt;/strong&gt; should be comparable. Therefore, they are often used to help identify compounds and &lt;a href=&#34;https://webbook.nist.gov/chemistry/&#34;&gt;NIST&lt;/a&gt; maintains a database of retention indeces for researchers to refer to.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/kowalsick table.png&#34; alt=&#34;An example table including literature retention indices from [Kowalsick, et al. 2014](https://doi.org/10.1016/j.chroma.2014.10.058)&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: An example table including literature retention indices from &lt;a href=&#34;https://doi.org/10.1016/j.chroma.2014.10.058&#34;&gt;Kowalsick, et al. 2014&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Producing such a table of literature retention indices for potentially hundreds of metabolites by hand can be really tedious!&lt;/p&gt;
&lt;p&gt;Enter &lt;code&gt;nist_ri()&lt;/code&gt;, a handy function I wrote to scrape retention index tables from NIST. Below, I work through an example of how you might use it. First, you need to install the latest version of &lt;code&gt;webchem&lt;/code&gt;. My function isn’t in the latest CRAN release at the time of writing this blog post, but you can install from github like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;ropensci/webchem&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To look up a retention index, you need a CAS identifier number for the chemical (For now, at least. Other search methods may be implemented in the future). If you don’t already have CAS numbers, you can get them using other functions in &lt;code&gt;webchem&lt;/code&gt; from chemical names or other identifier numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CASs &amp;lt;- c(&amp;quot;83-34-1&amp;quot;, &amp;quot;119-36-8&amp;quot;, &amp;quot;123-35-3&amp;quot;, &amp;quot;19700-21-1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the package and take a look at the help file. You’ll see that we need to choose what type of retention index to scrape, what polarity of column, and what kind of teperature program. If you browse one of the &lt;a href=&#34;https://webbook.nist.gov/cgi/cbook.cgi?ID=C78706&amp;amp;Units=SI&amp;amp;Mask=2000#Gas-Chrom&#34;&gt;NIST sites for a compound&lt;/a&gt;, this will make more sense.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(webchem)
?nist_ri&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s get Van Den Dool &amp;amp; Kratz (AKA “linear”) retention indeces for non-polar columns using a temperature ramp. This might take a while, depending on your internet connection and how many CAS numbers you request data for. If a certain type of retention index doesn’t exist for a compound, the function will return &lt;code&gt;NA&lt;/code&gt; for all columns but the CAS number.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RIs &amp;lt;- nist_ri(CASs, &amp;quot;linear&amp;quot;, &amp;quot;non-polar&amp;quot;, &amp;quot;ramp&amp;quot;)
head(RIs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       CAS      type  phase   RI length      gas substrate diameter
## 1 83-34-1 Capillary  SPB-5 1410     60     &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;     0.32
## 2 83-34-1 Capillary DB-5MS 1380     30   Helium      &amp;lt;NA&amp;gt;     0.25
## 3 83-34-1 Capillary  SE-54 1410     50   Helium      &amp;lt;NA&amp;gt;     0.32
## 4 83-34-1 Capillary   DB-5 1381     30 Hydrogen      &amp;lt;NA&amp;gt;     0.25
## 5 83-34-1 Capillary   DB-5 1389     30 Nitrogen      &amp;lt;NA&amp;gt;     0.25
## 6 83-34-1 Capillary DB-5MS 1399     30     &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;     0.25
##   thickness temp_start temp_end temp_rate hold_start hold_end
## 1      1.00         40      230         3          2       10
## 2      0.25         35      225        10          5       25
## 3        NA         40      240         8          2        5
## 4      0.25         35      270         5         NA       20
## 5      0.25         30      250         3          2        2
## 6      0.25         40      200        10          3       20
##                               reference comment
## 1                 Engel and Ratel, 2007 MSDC-RI
## 2   Lozano P.R., Drake M., et al., 2007 MSDC-RI
## 3    Schlutt B., Moran N., et al., 2007 MSDC-RI
## 4            Alves, Pinto, et al., 2005 MSDC-RI
## 5 Colahan-Sederstrom and Peterson, 2005 MSDC-RI
## 6  Whetstine, Cadwallader, et al., 2005 MSDC-RI&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see there are multiple retention indices (&lt;code&gt;RI&lt;/code&gt;) for each CAS number. Let’s filter this down some more using some functions from &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(stringr)
RIs_filtered &amp;lt;-
  RIs %&amp;gt;%
  filter(gas == &amp;quot;Helium&amp;quot;,
         between(length, 20, 30),
         str_detect(phase, &amp;quot;5&amp;quot;),
         diameter &amp;lt; 0.3,
         thickness == 0.25)
head(RIs_filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        CAS      type  phase     RI length    gas substrate diameter
## 1  83-34-1 Capillary DB-5MS 1380.0     30 Helium      &amp;lt;NA&amp;gt;     0.25
## 2  83-34-1 Capillary DB-5MS 1396.0     30 Helium      &amp;lt;NA&amp;gt;     0.25
## 3  83-34-1 Capillary   DB-5 1391.0     30 Helium      &amp;lt;NA&amp;gt;     0.26
## 4 119-36-8 Capillary   DB-5 1201.0     25 Helium      &amp;lt;NA&amp;gt;     0.25
## 5 119-36-8 Capillary HP-5MS 1200.7     30 Helium      &amp;lt;NA&amp;gt;     0.25
## 6 119-36-8 Capillary HP-5MS 1190.0     30 Helium      &amp;lt;NA&amp;gt;     0.25
##   thickness temp_start temp_end temp_rate hold_start hold_end
## 1      0.25         35      225        10          5       25
## 2      0.25         35      200        10          5       30
## 3      0.25         50      300         6          4       20
## 4      0.25         60      200         2         NA       60
## 5      0.25         80      300         4         NA       NA
## 6      0.25         60      280         4          5       NA
##                                 reference comment
## 1     Lozano P.R., Drake M., et al., 2007 MSDC-RI
## 2 Karagül-Yüceer, Vlahovich, et al., 2003 MSDC-RI
## 3                Rostad and Pereira, 1986 MSDC-RI
## 4                 Rout, Rao, et al., 2007 MSDC-RI
## 5                Zeng, Zhao, et al., 2007 MSDC-RI
## 6         Saroglou, Dorizas, et al., 2006 MSDC-RI&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we could &lt;code&gt;summarise&lt;/code&gt; to get an average of all the database entries…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RIs_filtered %&amp;gt;% 
  group_by(CAS) %&amp;gt;% 
  summarise(mean_RI = mean(RI))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   CAS        mean_RI
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 119-36-8     1193.
## 2 123-35-3      990.
## 3 19700-21-1   1430 
## 4 83-34-1      1389&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or if we wanted to pick a single entry for each CAS number with the median RI, we could do that as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_RIs &amp;lt;-
  RIs_filtered %&amp;gt;%
  group_by(CAS) %&amp;gt;% 
  filter(RI == median(RI)) %&amp;gt;% 
  filter(row_number() == 1) %&amp;gt;% 
  select(CAS, RI, reference)
best_RIs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 3
## # Groups:   CAS [4]
##   CAS           RI reference                             
##   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                                 
## 1 83-34-1     1391 Rostad and Pereira, 1986              
## 2 119-36-8    1191 Aligiannis, Kalpoutzakis, et al., 2004
## 3 123-35-3     991 Maccioni, Baldini, et al., 2007       
## 4 19700-21-1  1430 Dickschat, Wenzel, et al., 2004&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You could then easily take this table and &lt;code&gt;*_join()&lt;/code&gt; it to your GC/MS data, if you have a column for CAS#, and select the &lt;code&gt;RI&lt;/code&gt; and &lt;code&gt;reference&lt;/code&gt; columns, for example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fake.data &amp;lt;-
  data.frame(CAS = CASs,
             #Name = cts_convert(CASs, from = &amp;quot;CAS&amp;quot;, to = &amp;quot;Chemical Name&amp;quot;, first = TRUE),
             Name = c(&amp;quot;skatole&amp;quot;, &amp;quot;methyl salicylate&amp;quot;, &amp;quot;beta-myrcene&amp;quot;, &amp;quot;geosmin&amp;quot;),
             group_1_conc = round(abs(rnorm(4)), 3),
             group_2_conc = round(abs(rnorm(4)), 3))

left_join(fake.data, best_RIs) %&amp;gt;%
  select(CAS, Name, RI, everything()) %&amp;gt;% 
  arrange(RI) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;CAS&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;RI&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;group_1_conc&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;group_2_conc&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;123-35-3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;beta-myrcene&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;991&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.487&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.182&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Maccioni, Baldini, et al., 2007&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;119-36-8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;methyl salicylate&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1191&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.113&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.563&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aligiannis, Kalpoutzakis, et al., 2004&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;83-34-1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;skatole&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1391&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.142&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.972&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Rostad and Pereira, 1986&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;19700-21-1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;geosmin&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1430&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.174&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.783&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Dickschat, Wenzel, et al., 2004&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Python is Weird (an unabashedly biased intro to Python for R users)</title>
      <link>http://ericrscott.com/2018/05/03/python-is-weird/</link>
      <pubDate>Thu, 03 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2018/05/03/python-is-weird/</guid>
      <description>

&lt;p&gt;Last semester I took a class that used Python. It was my first time really seriously using any programing language other than R. The students were about half engineers and half biologists.  The vast majority of the biologists knew R to varying degrees, but had no experience with Python, and the engineers seemed to generally have some experience with Python, or at least with languages more similar to it than R. I wish that the instructor could have taught every Python lecture like &amp;ldquo;Ok, today we&amp;rsquo;re going to learn the Python equivalent of doing ____ in R&amp;rdquo;, but of course that wouldn&amp;rsquo;t be fair to about half the students.&lt;/p&gt;

&lt;p&gt;So for anyone else making the leap from R to Python, here are three things that are going to feel really weird about Python.&lt;/p&gt;

&lt;h1 id=&#34;1-indexing-is-not-intuitive-in-python&#34;&gt;1. Indexing is not intuitive in Python&lt;/h1&gt;

&lt;p&gt;Let me just show you first and see if you can figure out what is going on:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;R Code:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# R
x = c(10, 20, 30, 40, 50)
x[1]
x[1:3]
x[4:5]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 10
## [1] 10 20 30
## [1] 40 50
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cool, cool.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Equivalent in Python:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python
x = [10, 20, 30, 40, 50]
print(x[1])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 20
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 10
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x[3:4])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [40]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x[3:5])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [40, 50]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait, what? Two things are really weird about this.  First, the first position in the vector is not position 1, it is position 0.  Second, &lt;code&gt;x[3:4]&lt;/code&gt; returns only a single number.  Why?!  Because in Python, the second number in the index is not inclusive, so if you want to get the 4th and 5th values of &lt;code&gt;x&lt;/code&gt; (index positions 3 and 4 in Python world), then you have to use &lt;code&gt;x[3:5]&lt;/code&gt; &lt;strong&gt;even though there is NO POSITION 5&lt;/strong&gt;.  Terrible.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Weird thing 1.1: Python is much more geared toward writing programs than R.  That means you can&amp;rsquo;t really run python code one line at a time like R and you have to explicitly &lt;code&gt;print()&lt;/code&gt; things that you want to be output to the screen.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&#34;2-you-need-to-load-a-package-just-to-do-vector-math&#34;&gt;2. You need to load a package just to do vector math&lt;/h1&gt;

&lt;p&gt;R is built for doing math and statistics, so vectors and matrices are built in and you can do math on them!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;R Code:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# R
x = c(1, 2, 3)
x + 10
x * 2
y = c(5, 6, 7)
x + y
#Yay vector arithmetic!
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 11 12 13
## [1] 2 4 6
## [1]  6  8 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python is &lt;strong&gt;not&lt;/strong&gt; built with math and statistics in mind, and this doesn&amp;rsquo;t work without using a package.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Equivalent in Python:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python
x = [1, 2, 3]
print(x + [10]) 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1, 2, 3, 10]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x*3) 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1, 2, 3, 1, 2, 3, 1, 2, 3]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = [5, 6, 7]
print(x + y) 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1, 2, 3, 5, 6, 7]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clearly &lt;code&gt;+&lt;/code&gt; is doing something different in base Python&amp;mdash;it&amp;rsquo;s concatenating &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;10&lt;/code&gt;.  Similarly, &lt;code&gt;*&lt;/code&gt; is not multiplying, but concatenating three &lt;code&gt;x&lt;/code&gt;s in a row. This is completely ridiculous behavior for numbers, but when you&amp;rsquo;re working with strings, it&amp;rsquo;s actually pretty freakin&amp;rsquo; great.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python
print((&amp;quot;Yay &amp;quot;+&amp;quot;Python! &amp;quot;) * 5)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Yay Python! Yay Python! Yay Python! Yay Python! Yay Python!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want numerical vectors to work like they should, you have to use a special kind of vector called a &lt;strong&gt;numpy array&lt;/strong&gt;.  Numpy is a package for Python that provides a bunch of functions that work on numbers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python
import numpy as np
x = np.array([1, 2, 3])
print(x + 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [11 12 13]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x * 2) 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [2 4 6]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = np.array([5, 6, 7])
print(x + y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [ 6  8 10]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you do math to numpy arrays, you get what you&amp;rsquo;d expect as an R user.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Wierd thing 2.1: note that the &lt;code&gt;packagename.function()&lt;/code&gt; form is equivalent to &lt;code&gt;packagename::function()&lt;/code&gt; in R, but unlike R, it is always required.  That is, as far as I know, there is nothing you can do to make &lt;code&gt;array([1,2,3])&lt;/code&gt; work without the preceding &lt;code&gt;np.&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&#34;3-default-assignment-behavior-is-aliasing&#34;&gt;3. Default assignment behavior is aliasing&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m still trying to wrap my mind around this one, so rather than trying to explain it, let me show you an example first:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;R Code:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# R
a1 = c(1,2,3)
a2 = a1
a2[1] = 100
a1
a2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 1 2 3
## [1] 100   2   3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;a1&lt;/code&gt; is, of course, unchanged by changing a value in &lt;code&gt;a2&lt;/code&gt;.  Let&amp;rsquo;s see if that&amp;rsquo;s true in Python.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Equivalent in Python:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python
import numpy as np
a1 = np.array([1,2,3])
a2 = a1
a2[1] = 100
print(a1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [  1 100   3]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(a2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [  1 100   3]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Changing a value in &lt;code&gt;a2&lt;/code&gt; &lt;em&gt;changes&lt;/em&gt; the same value in &lt;code&gt;a1&lt;/code&gt;! In this case, &lt;code&gt;a2&lt;/code&gt; is an &lt;em&gt;alias&lt;/em&gt; of &lt;code&gt;a1&lt;/code&gt;, not a copy. This only happens when you do &lt;code&gt;object1 = object2&lt;/code&gt; and not when you do something to &lt;code&gt;object2&lt;/code&gt; as you&amp;rsquo;re assigning it. Here&amp;rsquo;s another example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python
import numpy as np
a1 = np.array([1,2,3])
a2 = a1 + 2
a2[1] = 100
print(a1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1 2 3]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(a2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [  3 100   5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now &lt;code&gt;a2&lt;/code&gt; is a separate object from &lt;code&gt;a1&lt;/code&gt; instead of just an alias. If you want to make an &lt;em&gt;exact&lt;/em&gt; copy, you have to do that explicitly with &lt;code&gt;a2 = np.copy(a1)&lt;/code&gt; or &lt;code&gt;a2 = a1[:]&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&#34;try-python&#34;&gt;Try Python!&lt;/h1&gt;

&lt;p&gt;As many people in the data science world have pointed out, it&amp;rsquo;s not R vs. Python, it&amp;rsquo;s &lt;a href=&#34;https://www.datasciencecentral.com/profiles/blogs/r-vs-python-r-and-python-and-something-else&#34; target=&#34;_blank&#34;&gt;R &lt;em&gt;and&lt;/em&gt; Python&lt;/a&gt;. From my limited experience, the benefits of Python over R I&amp;rsquo;ve are that it seems to be faster, defining classes and functions seems less painful, and it&amp;rsquo;s great at working with strings out of the box. I don&amp;rsquo;t really plan on working in Python more unless I have to, but knowing a bit of the language will be useful for talking shop with people who use it!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhDrinking Podcast</title>
      <link>http://ericrscott.com/2018/05/01/phdrinking-podcast/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2018/05/01/phdrinking-podcast/</guid>
      <description>&lt;p&gt;I had the wonderful opportunity to be on Sadie Wit&amp;rsquo;s &lt;a href=&#34;https://soundcloud.com/phdrinking&#34; target=&#34;_blank&#34;&gt;PhDrinking podcast&lt;/a&gt;.  Check out the episode &lt;a href=&#34;https://soundcloud.com/phdrinking/tea-totaling&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and subscribe to PhDrinking wherever great podcasts are casted.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cupcake Update</title>
      <link>http://ericrscott.com/2018/04/10/cupcake-update/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2018/04/10/cupcake-update/</guid>
      <description>&lt;p&gt;I know you&amp;rsquo;re all waiting on the edge of your seats for an update on the &lt;a href=&#34;http://www.ericrscott.com/2018/03/05/cupcakes-vs-muffins/&#34; target=&#34;_blank&#34;&gt;cupcakes vs. muffins data science project&lt;/a&gt;, but unfortunately I don&amp;rsquo;t have any answers to that age-old question* yet.&lt;/p&gt;

&lt;p&gt;As silly as it may sound, I&amp;rsquo;m actually considering using this data set for a paper about using PLS (partial least squares regression) for ecological data. So for now, I&amp;rsquo;m holding off on blogging about any results of analyses in case I end up wanting to use them for the publication. In the meantime, the fully cleaned (I hope) &lt;a href=&#34;https://github.com/Aariq/cupcakes-vs-muffins&#34; target=&#34;_blank&#34;&gt;dataset is up on github&lt;/a&gt;, so feel free to play around with it. Once I have a general outline for this paper, I may feel more comfortable blogging about the results. Please comment if you think I&amp;rsquo;m being silly and overparanoid about holding back on this.&lt;/p&gt;

&lt;p&gt;*all I can say is muffins probably differ from cupcakes by more than just frosting, based on the ingredients.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cupcakes vs. Muffins</title>
      <link>http://ericrscott.com/2018/03/05/cupcakes-vs-muffins/</link>
      <pubDate>Mon, 05 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2018/03/05/cupcakes-vs-muffins/</guid>
      <description>&lt;p&gt;One thing I’ve learned from my PhD at Tufts is that I really enjoy working data wrangling, visualization, and statistics in R. I enjoy it so much, that lately I’ve been strongly considering a career in data science after graduation. As a way to showcase my data science skills, I’ve been working on a side project to use webscraping and multivariate statistics to answer the age old question: Are cupcakes really &lt;em&gt;that&lt;/em&gt; different from muffins?&lt;/p&gt;
&lt;p&gt;Honestly, I can’t even quite remember how this idea came to me, but it started in a discussion with Dr. Elizabeth Crone about why more ecologists don’t use a statistical technique called &lt;a href=&#34;https://en.wikipedia.org/wiki/Partial_least_squares_regression&#34;&gt;partial least squares regression&lt;/a&gt;. We wanted a fun multivariate data set that could illustrate the different conclusions you might get depending on the statistical method you use. Around the same time, I came across a blog post by &lt;a href=&#34;https://aczane.netlify.com/2018/02/08/the-first-and-namesake-post-is-it-cake/&#34;&gt;@lariebyrd&lt;/a&gt; explaining machine learning using cake emoji. And that somehow led to me webscraping &lt;strong&gt;every single muffin and cupcake recipe&lt;/strong&gt; on allrecipes.com.&lt;/p&gt;
&lt;p&gt;I just finished the webscraping bit of the project this weekend. I’m not going to reproduce the code here, but rather address some of the challenges I faced, and some things I’ve learned so far. You can check out my R notebook and a .rds file of all the recipes &lt;a href=&#34;https://github.com/Aariq/cupcakes-vs-muffins&#34;&gt;over on github&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;getting-started-on-webscraping&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Getting started on webscraping&lt;/h1&gt;
&lt;p&gt;I followed this &lt;a href=&#34;https://towardsdatascience.com/web-scraping-tutorial-in-r-5e71fd107f32&#34;&gt;wonderful tutorial&lt;/a&gt; from José Roberto Ayala Solares to get going. Necessary tools include the &lt;code&gt;tidyverse&lt;/code&gt;, the &lt;code&gt;rvest&lt;/code&gt; package for webscraping, and a chrome plugin called &lt;a href=&#34;http://selectorgadget.com/&#34;&gt;SelectorGadget&lt;/a&gt;. Going through the example in the tutorial was really helpful for getting a hang of what is and isn’t easy/possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-data-source&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Choosing a data source&lt;/h1&gt;
&lt;p&gt;I specifically chose allrecipes.com over, say geniuskitchen.com (an excellent recipe site) because of the way it categorizes and structures recipes. When I search “cupcake” on geniuskitchen.com, I get 1830 results (yay!), but a bunch of them are links to videos, articles, reviews, blog posts, food porn albums, and other things that are &lt;strong&gt;not&lt;/strong&gt; recipes (boo!). Allrecipes.com, on the other hand, gives me the following URL: &lt;a href=&#34;https://www.allrecipes.com/recipes/377/desserts/cakes/cupcakes/&#34; class=&#34;uri&#34;&gt;https://www.allrecipes.com/recipes/377/desserts/cakes/cupcakes/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is &lt;strong&gt;ALL THE CUPCAKES&lt;/strong&gt;. From there, I played around with SelectorGadget to make sure it was going to be easy to drill down to just the links to the actual recipes, and yes, it was.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:selector-gadget&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/selector-gadget.png&#34; alt=&#34;SelectorGadget in action&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: SelectorGadget in action
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I also checked that the ingredients list of each recipe was going to be easy to scrape. They all have the same format, and some brief testing convinced me that I’d be able to figure out how to pull out ingredients.&lt;/p&gt;
&lt;p&gt;The takeaway is that for this project I had my choice of many websites, but I specifically picked one that would make my life easier because of its html structure.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dont-scrape-too-fast&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Don’t scrape too fast!&lt;/h1&gt;
&lt;div id=&#34;sys.sleep-is-your-friend&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;(&lt;code&gt;Sys.sleep()&lt;/code&gt; is your friend)&lt;/h2&gt;
&lt;p&gt;So once I figured out how to pull in all the links to all the cupcake recipes, I started scraping ingredients from a small sample of them. After a few debugging runs, allrecipes.com stopped responding and I just kept getting error messages. After pulling my hair out trying to figure out how I broke my code, I realized that my IP was being blocked! Because of the speed of accessing the website or how many links I accessed, my IP was suspected of being a bot or something and was temporarily (&lt;em&gt;whew&lt;/em&gt;) blocked. I turned to twitter and was reccomended an easy fix—create a custom &lt;code&gt;read_html_slow()&lt;/code&gt; function that included &lt;code&gt;Sys.sleep(5)&lt;/code&gt; which just makes R wait 5 seconds in between reading websites.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Make your own function with read_html followed by Sys.sleep(5) and then map using that?&lt;/p&gt;&amp;mdash; Sharon Machlis (@sharon000) &lt;a href=&#34;https://twitter.com/sharon000/status/965808697346789378?ref_src=twsrc%5Etfw&#34;&gt;February 20, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
read_html_slow &amp;lt;- function(x, ...){
  output &amp;lt;- read_html(x)
  Sys.sleep(5) #wait 5 seconds before returning output
  return(output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;create-your-own-custom-helper-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Create your own custom helper functions&lt;/h1&gt;
&lt;p&gt;A great side-effect of creating your own functions like &lt;code&gt;read_html_slow()&lt;/code&gt; is making your code more readable. Instead of a for-loop that calls &lt;code&gt;Sys.sleep(5)&lt;/code&gt; after every iteration of &lt;code&gt;read_html()&lt;/code&gt;, I now have one function that does it all and can be easily used in conjunction with &lt;code&gt;map()&lt;/code&gt; from the &lt;code&gt;purrr&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;My &lt;code&gt;read_html_slow()&lt;/code&gt; function would still occasionally encounter errors like when it would encounter a broken URL. When reading in a whole list of URLs, one broken URL would mess up the whole list. I ended up expanding on &lt;code&gt;read_html_slow()&lt;/code&gt; to make &lt;code&gt;read_html_safely()&lt;/code&gt; which would output an &lt;code&gt;NA&lt;/code&gt; rather than throwing an error if a URL was broken.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)
read_html_safely &amp;lt;- possibly(read_html_slow, NA) #from the purrr package&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also created a &lt;code&gt;str_detect_any()&lt;/code&gt; which allows you to check if a string is matched by any of a vector of regular expressions. I show how I use this in the next section&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringi)
str_detect_any &amp;lt;- function(string, pattern){
  map_lgl(string, ~stri_detect_regex(., pattern) %&amp;gt;% any(.)) 
  #map_lgl is from purrr, stri_detect() is from stringi
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;work-on-random-samples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Work on random samples&lt;/h1&gt;
&lt;p&gt;There were something like 200 cupcake recipes and another 100 muffin recipes on allrecipes.com, which takes a long time to read in. Rather than working on the whole data set, I used &lt;code&gt;sample()&lt;/code&gt; on my vector of recipe URLs to take a manageable sample of recipes to work on. After working through a few different random subsets, I reached a point where I was happy with how my code was working. Only then did I read in the entire data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;webscraped-data-is-messy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Webscraped data is messy&lt;/h1&gt;
&lt;div id=&#34;people-make-weird-baked-goods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;(People make weird baked goods)&lt;/h2&gt;
&lt;p&gt;Once I had all my ingredients for muffins and cupcakes in a data frame, I needed to standardize the ingredients. For example “8 tablespoons butter, melted” and “1/2 cup unsalted, organic, non-GMO, gluten-free, single-origin butter” both needed to get converted to “1/2 cup butter.” This is where the combination of &lt;code&gt;mutate()&lt;/code&gt;, &lt;code&gt;case_when()&lt;/code&gt; and &lt;code&gt;str_detect()&lt;/code&gt; really came in handy to make readable, debuggable code.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mutate()&lt;/code&gt; is a function from the &lt;code&gt;dplyr&lt;/code&gt; package (part of &lt;code&gt;tidyverse&lt;/code&gt;) for adding new columns to data frames based on information in other columns. Here, I used it to take the ingredient descriptions and turn them into short, concise ingredients. &lt;code&gt;str_detect()&lt;/code&gt; is from the &lt;code&gt;stringr&lt;/code&gt; package and takes a string and a regular expression pattern and outputs &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;. Finally, &lt;code&gt;case_when()&lt;/code&gt; is also from &lt;code&gt;dplyr&lt;/code&gt; and provides a readable alternative to insane nested &lt;code&gt;ifelse()&lt;/code&gt; statements. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
df &amp;lt;- tibble(description = c(&amp;quot;1/2 cup unsalted, organic, non-GMO, gluten-free, single-origin butter&amp;quot;,
                             &amp;quot;1 cup buttermilk&amp;quot;,
                             &amp;quot;4 cups sugar&amp;quot;,
                             &amp;quot;4 cups slivered almonds&amp;quot;,
                             &amp;quot;1/2 cup chopped walnuts&amp;quot;,
                             &amp;quot;1 teaspoon salt&amp;quot;,
                             &amp;quot;25 blueberries&amp;quot;))

#all nuts should match one of these patterns
nuts &amp;lt;- c(&amp;quot;almond&amp;quot;, &amp;quot;\\w*nut&amp;quot;, &amp;quot;pecan&amp;quot;)

df %&amp;gt;%
  mutate(ingredient = case_when(str_detect(.$description, &amp;quot;butter&amp;quot;) ~  &amp;quot;butter&amp;quot;,
                                str_detect(.$description, &amp;quot;milk&amp;quot;)   ~  &amp;quot;milk&amp;quot;,
                                str_detect(.$description, &amp;quot;sugar&amp;quot;)  ~  &amp;quot;sugar&amp;quot;,
                                str_detect(.$description, &amp;quot;salt&amp;quot;)   ~  &amp;quot;salt&amp;quot;,
                                str_detect_any(.$description, nuts) ~  &amp;quot;nut&amp;quot;,
                                TRUE                                ~  as.character(NA)
                                )
         )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   description                                                    ingredient
##   &amp;lt;chr&amp;gt;                                                          &amp;lt;chr&amp;gt;     
## 1 1/2 cup unsalted, organic, non-GMO, gluten-free, single-origi… butter    
## 2 1 cup buttermilk                                               butter    
## 3 4 cups sugar                                                   sugar     
## 4 4 cups slivered almonds                                        nut       
## 5 1/2 cup chopped walnuts                                        nut       
## 6 1 teaspoon salt                                                salt      
## 7 25 blueberries                                                 &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The way &lt;code&gt;case_when()&lt;/code&gt; works is just like a bunch of nested &lt;code&gt;ifelse()&lt;/code&gt; statements. That is, if it satisfies a condition on the left of the &lt;code&gt;~&lt;/code&gt; in the first line, it returns the output to the right, otherwise it goes to the next line. That results in “buttermilk” getting categorized as “butter”. If you wanted it to be captured as “milk” instead, you could switch the order of the butter and milk lines inside &lt;code&gt;case_when()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I had to do this sort of thing &lt;strong&gt;a lot&lt;/strong&gt;. For example, when “creamy peanut butter” was getting categorized as “cream” or “butter” instead of “nuts” or when “unsalted butter” was getting categorized as “salt”. You’ll also notice that if a description makes it all the way through the list, it gets categorized as &lt;code&gt;NA&lt;/code&gt;. I’ll never be able to categorize all the cupcake/muffin ingredients, because people put &lt;a href=&#34;https://www.allrecipes.com/recipe/215561/peanut-butter-bacon-cupcake/&#34;&gt;weird shit&lt;/a&gt; in their baked goods.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Webscraping can be frustrating, but you can set yourself up for success by choosing an easily scrapable website, annotating your code as you go, and taking measures to make your code as readable as possible. With big, messy data, you’ll likely never get it perfect, but you can use random samples of websites to help debug your code and test its effectiveness on new random samples of websites.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Next Steps&lt;/h1&gt;
&lt;p&gt;Now that I have a data set I’m pretty happy with, the next step of the project is to do some exploratory data analysis to see what properties it has that are relevant to the sorts of multivariate data that ecologists deal with. Then on to statistical analyses to figure out what ingredients make cupcakes different from muffins. Is it sweetness? Is it something to do with leavening? Butter vs oil? Leave your predictions in the comments below!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Importing data from a LI-COR photosynthesis meter into R</title>
      <link>http://ericrscott.com/2018/01/17/li-cor-wrangling/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2018/01/17/li-cor-wrangling/</guid>
      <description>&lt;script src=&#34;http://ericrscott.com/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://ericrscott.com/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://ericrscott.com/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://ericrscott.com/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://ericrscott.com/rmarkdown-libs/str_view/str_view.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://ericrscott.com/rmarkdown-libs/str_view-binding/str_view.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The &lt;a href=&#34;https://www.licor.com/env/products/photosynthesis/LI-6400XT/&#34;&gt;LI-6400XT&lt;/a&gt; is a portable device used to measure photosynthesis in plant leaves. As you take measurements by pressing a button on the device, they are recorded into memory. In order to keep track of which measurments go with which plants (or experimental treatments), there is an “add remark” option where you can enter sample information before taking measurements.&lt;/p&gt;
&lt;p&gt;When the data are exported, you get a series of .xls files and a plain text file. Both of these have some problems that you’ll have to deal with if you want to read the data into R and use it for statistical analysis or generating reports.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:excel&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/licor_excel.png&#34; alt=&#34;Excel nightmare or text nightmare? Pick your poison.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Excel nightmare or text nightmare? Pick your poison.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Both file types create some problems for easily getting the data into R:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Header information&lt;/strong&gt; interrupts the data table format. Fortunatly, it’s mostly just information about the instrument configuration that we don’t need.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Untidy handling of remarks&lt;/strong&gt;. Instead of remarks being in their own column, they appear in the &lt;code&gt;HHMMSS&lt;/code&gt; column in the .xls files and in the &lt;code&gt;Obs&lt;/code&gt; column in the .txt file! And to indicate that the row is a remark, instead of giving it an observation number in &lt;code&gt;Obs&lt;/code&gt;, it just says “Remark =”.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Column headers are spread over two rows&lt;/strong&gt;. There is a (somewhat mysterious to me) row of “in”s and “out”s under the column headers in the .xls file.&lt;/li&gt;
&lt;li&gt;Another problem that you can’t see in Fig. 1 is that I’ve done my &lt;strong&gt;measurments in several bouts&lt;/strong&gt;. This produced two .xls files and a text file with header text inbetween my two sets of measurements.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point I had to choose between reading in the .xls files with &lt;code&gt;read_xls()&lt;/code&gt; from the &lt;code&gt;readxl&lt;/code&gt; package and doing some wrangling from there, or to deal with the text file, which would surely include some regular expression headaches.&lt;/p&gt;
&lt;p&gt;For some unknown reason, &lt;code&gt;read_xls()&lt;/code&gt; didn’t work on these files, and I had to open them in Excel, then save them as .xlsx files and use &lt;code&gt;read_xlsx()&lt;/code&gt; to get them into R. For the sake of full automatedness, I’m going to work through the text file example here.&lt;/p&gt;
&lt;div id=&#34;tidying-up-raw-text&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidying up raw text&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:flowchart&#34;&gt;&lt;/span&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph rmarkdown {\n        \n        graph [layout = dot, rankdir = TB, fontsize = 16]\n        # node definitions with substituted label text\n        node [fontname = Avenir, shape = rectangle]        \n        rec1 [label = \&#34;Raw text\&#34;]\n        rec2 [label = \&#34;List of strings\&#34;]\n        rec3 [label = \&#34;List of data frames\&#34;]\n        rec4 [label = \&#34;Extract sample ID\nfrom remarks\&#34;]\n\n        node [fontname = Avenir, shape = oval]\n        ova1 [label = \&#34;Tidy and split\&#34;]\n\n        node [fontname = Courier, shape = oval]\n        ova2 [label = \&#34;map(list, read_tsv)\&#34;]\n        ova3 [label = \&#34;bind_rows()\&#34;]\n\n        # edge definitions with the node IDs\n        rec1 -&gt; ova1 -&gt; rec2 -&gt; ova2 -&gt; rec3 -&gt; ova3 -&gt; rec4\n  }&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: My approach to wrangling text files generated by the LI-6400XT
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;My approach is to read in the raw text, tidy it up, then use &lt;code&gt;read_tsv()&lt;/code&gt; to get a list of data frames. After that, I planned to combine them into one big data frame and do some more tidying to extract the sample IDs from the remarks. I’ll be using functions from &lt;code&gt;stringr&lt;/code&gt; to do the text tidying, and functions from various &lt;code&gt;tidyverse&lt;/code&gt; packages to bring it all together into a coherent data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(stringr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text.raw &amp;lt;- read_file(&amp;quot;licor.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Scrolling through the text a little reveals that, conveniently, the line &lt;code&gt;&#34;$STARTOFDATA$&#34;&lt;/code&gt; appears between the header information and the start of the actual data. The headers themselves always begin with &lt;code&gt;&#34;OPEN&#34;&lt;/code&gt; followed by a date. I created regular expression patterns for these and used them to split the raw text file first into separate bouts of measurements, then into headers and data, discarding the headers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;header_pattern &amp;lt;- &amp;quot;\&amp;quot;OPEN \\d\\.\\d\\.\\d&amp;quot;
data_pattern &amp;lt;- &amp;quot;\\$STARTOFDATA\\$&amp;quot;

#splits into individual bouts
raw_split &amp;lt;- str_split(text.raw, header_pattern, simplify = TRUE)

#splits further to separate headers from actual data
raw_split2 &amp;lt;- str_split(raw_split, data_pattern, simplify = FALSE)

str(raw_split2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 3
##  $ : chr &amp;quot;&amp;quot;
##  $ : chr [1:2] &amp;quot;\&amp;quot;\n\&amp;quot;Fri Aug 25 2017 08:29:30\&amp;quot;\n&amp;lt;open&amp;gt;&amp;lt;version&amp;gt;\&amp;quot;6.3.4\&amp;quot;&amp;lt;/version&amp;gt;&amp;lt;/open&amp;gt;\n&amp;lt;open&amp;gt;&amp;lt;configfile&amp;gt;\&amp;quot;/User/Configs/&amp;quot;| __truncated__ &amp;quot;\n\&amp;quot;Obs\&amp;quot;\t\&amp;quot;HHMMSS\&amp;quot;\t\&amp;quot;FTime\&amp;quot;\t\&amp;quot;EBal?\&amp;quot;\t\&amp;quot;Photo\&amp;quot;\t\&amp;quot;Cond\&amp;quot;\t\&amp;quot;Ci\&amp;quot;\t\&amp;quot;Trmmol\&amp;quot;\t\&amp;quot;VpdL\&amp;quot;\t\&amp;quot;CTleaf\&amp;quot;\t\&amp;quot;A&amp;quot;| __truncated__
##  $ : chr [1:2] &amp;quot;\&amp;quot;\n\&amp;quot;Fri Aug 25 2017 10:32:44\&amp;quot;\n&amp;lt;open&amp;gt;&amp;lt;version&amp;gt;\&amp;quot;6.3.4\&amp;quot;&amp;lt;/version&amp;gt;&amp;lt;/open&amp;gt;\n&amp;lt;open&amp;gt;&amp;lt;configfile&amp;gt;\&amp;quot;/User/Configs/&amp;quot;| __truncated__ &amp;quot;\n\&amp;quot;Obs\&amp;quot;\t\&amp;quot;HHMMSS\&amp;quot;\t\&amp;quot;FTime\&amp;quot;\t\&amp;quot;EBal?\&amp;quot;\t\&amp;quot;Photo\&amp;quot;\t\&amp;quot;Cond\&amp;quot;\t\&amp;quot;Ci\&amp;quot;\t\&amp;quot;Trmmol\&amp;quot;\t\&amp;quot;VpdL\&amp;quot;\t\&amp;quot;CTleaf\&amp;quot;\t\&amp;quot;A&amp;quot;| __truncated__&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s a little hard to see here, but now there is a list of 3 elements. The first element contains nothing (because there is nothing before the first &lt;code&gt;header_pattern&lt;/code&gt;), the other elements contain two strings—one is the header, the other is the data. Let’s get rid of the headers and the empty list element.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#extract just the second element, the actual data
raw_split3 &amp;lt;- raw_split2 %&amp;gt;%
  map(`[`, 2) %&amp;gt;% #equivalent to doing raw_split2[[i]][2] for every element &amp;quot;i&amp;quot;
  flatten_chr() #converts to a vector

#remove empty elements
raw_split3 &amp;lt;- raw_split3[!is.na(raw_split3)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-in-our-cleaned-text-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading in our cleaned text file&lt;/h2&gt;
&lt;p&gt;Then we can finally read in our cleaned text as a tab-separated (.tsv) file. Here I make use of &lt;code&gt;map()&lt;/code&gt; from the &lt;code&gt;purrr&lt;/code&gt; package to apply &lt;code&gt;read_tsv()&lt;/code&gt; to every string in our raw text vector. &lt;code&gt;skip = 1&lt;/code&gt; gets rid of that weird line of “in”s and “out”s.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;input &amp;lt;- raw_split3 %&amp;gt;%
  map(read_tsv, skip = 1)

input.all &amp;lt;- bind_rows(input)
head(input.all, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 40
##    Obs   HHMMSS FTime `EBal?` Photo    Cond    Ci Trmmol  VpdL CTleaf  Area
##    &amp;lt;chr&amp;gt; &amp;lt;time&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 08:3…    NA     NA      NA NA    NA         NA NA     NA      NA      NA
##  2 08:3…    NA     NA      NA NA    NA         NA NA     NA      NA      NA
##  3 08:4…    NA     NA      NA NA    NA         NA NA     NA      NA      NA
##  4 08:4…    NA     NA      NA NA    NA         NA NA     NA      NA      NA
##  5 08:4…    NA     NA      NA NA    NA         NA NA     NA      NA      NA
##  6 08:4…    NA     NA      NA NA    NA         NA NA     NA      NA      NA
##  7 08:4…    NA     NA      NA NA    NA         NA NA     NA      NA      NA
##  8 08:4…    NA     NA      NA NA    NA         NA NA     NA      NA      NA
##  9 1     08:43    932       0  6.62  0.0589   202  0.724  1.25   22.3     6
## 10 2     08:43    937       0  6.99  0.0594   193  0.731  1.25   22.3     6
## # … with 29 more variables: BLC_1 &amp;lt;dbl&amp;gt;, StmRat &amp;lt;dbl&amp;gt;, BLCond &amp;lt;dbl&amp;gt;,
## #   Tair &amp;lt;dbl&amp;gt;, Tleaf &amp;lt;dbl&amp;gt;, TBlk &amp;lt;dbl&amp;gt;, CO2R &amp;lt;dbl&amp;gt;, CO2S &amp;lt;dbl&amp;gt;,
## #   H2OR &amp;lt;dbl&amp;gt;, H2OS &amp;lt;dbl&amp;gt;, RH_R &amp;lt;dbl&amp;gt;, RH_S &amp;lt;dbl&amp;gt;, Flow &amp;lt;dbl&amp;gt;,
## #   PARi &amp;lt;dbl&amp;gt;, PARo &amp;lt;dbl&amp;gt;, Press &amp;lt;dbl&amp;gt;, CsMch &amp;lt;dbl&amp;gt;, HsMch &amp;lt;dbl&amp;gt;,
## #   CsMchSD &amp;lt;dbl&amp;gt;, HsMchSD &amp;lt;dbl&amp;gt;, CrMchSD &amp;lt;dbl&amp;gt;, HrMchSD &amp;lt;dbl&amp;gt;,
## #   StableF &amp;lt;dbl&amp;gt;, BLCslope &amp;lt;dbl&amp;gt;, BLCoffst &amp;lt;dbl&amp;gt;, f_parin &amp;lt;dbl&amp;gt;,
## #   f_parout &amp;lt;dbl&amp;gt;, alphaK &amp;lt;dbl&amp;gt;, Status &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;extracting-useful-remarks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extracting useful remarks&lt;/h2&gt;
&lt;p&gt;The first step is moving remarks into a &lt;code&gt;remark&lt;/code&gt; column while keeping the observation numbers in the &lt;code&gt;Obs&lt;/code&gt; column. I’m sure there is a more elegant way to do this, but I had recently &lt;a href=&#34;http://purrr.tidyverse.org/reference/safely.html&#34;&gt;learned about&lt;/a&gt; the &lt;code&gt;safely()&lt;/code&gt; function from &lt;code&gt;purrr&lt;/code&gt; which allows you to capture errors. I figured I could try converting elements of the &lt;code&gt;Obs&lt;/code&gt; column to integers and if it failed, I could use that as a criteria for moving to a new column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create a &amp;quot;safe&amp;quot; version of as.integer() that returns a list of a result and error
safe_as.int &amp;lt;- safely(as.integer)
#returns error for text remarks, returns value for integer observation numbers

input.all &amp;lt;- input.all %&amp;gt;% 
  mutate(#create a comment column to indicate if an &amp;quot;Obs&amp;quot; is actually a remark
       comment = is.na(safe_as.int(Obs)$result), 
       #copy those remarks to the remark column
       remark = ifelse(comment == TRUE, Obs, NA),
       #remove remarks from Obs column
       Obs = ifelse(comment == FALSE, Obs, NA)) %&amp;gt;% 
#move the remark column the the begining
select(remark, everything()) %&amp;gt;% 
#remove the comment column.  We&amp;#39;re done with it
select(-comment)
head(input.all, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 41
##    remark Obs   HHMMSS FTime `EBal?` Photo    Cond    Ci Trmmol  VpdL
##    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;time&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 08:30… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA     NA   
##  2 08:32… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA     NA   
##  3 08:40… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA     NA   
##  4 08:40… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA     NA   
##  5 08:40… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA     NA   
##  6 08:40… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA     NA   
##  7 08:42… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA     NA   
##  8 08:43… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA     NA   
##  9 &amp;lt;NA&amp;gt;   1     08:43    932       0  6.62  0.0589   202  0.724  1.25
## 10 &amp;lt;NA&amp;gt;   2     08:43    937       0  6.99  0.0594   193  0.731  1.25
## # … with 31 more variables: CTleaf &amp;lt;dbl&amp;gt;, Area &amp;lt;dbl&amp;gt;, BLC_1 &amp;lt;dbl&amp;gt;,
## #   StmRat &amp;lt;dbl&amp;gt;, BLCond &amp;lt;dbl&amp;gt;, Tair &amp;lt;dbl&amp;gt;, Tleaf &amp;lt;dbl&amp;gt;, TBlk &amp;lt;dbl&amp;gt;,
## #   CO2R &amp;lt;dbl&amp;gt;, CO2S &amp;lt;dbl&amp;gt;, H2OR &amp;lt;dbl&amp;gt;, H2OS &amp;lt;dbl&amp;gt;, RH_R &amp;lt;dbl&amp;gt;,
## #   RH_S &amp;lt;dbl&amp;gt;, Flow &amp;lt;dbl&amp;gt;, PARi &amp;lt;dbl&amp;gt;, PARo &amp;lt;dbl&amp;gt;, Press &amp;lt;dbl&amp;gt;,
## #   CsMch &amp;lt;dbl&amp;gt;, HsMch &amp;lt;dbl&amp;gt;, CsMchSD &amp;lt;dbl&amp;gt;, HsMchSD &amp;lt;dbl&amp;gt;, CrMchSD &amp;lt;dbl&amp;gt;,
## #   HrMchSD &amp;lt;dbl&amp;gt;, StableF &amp;lt;dbl&amp;gt;, BLCslope &amp;lt;dbl&amp;gt;, BLCoffst &amp;lt;dbl&amp;gt;,
## #   f_parin &amp;lt;dbl&amp;gt;, f_parout &amp;lt;dbl&amp;gt;, alphaK &amp;lt;dbl&amp;gt;, Status &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the data frame above, you’ll notice that some of the remarks are just me changing parameters of the device, while others are sample IDs (e.g. “08:43:13 c 4 a” is plot c, plant 4, leaf a). I got lucky in my sample naming convention in that the sample IDs are relatively easily distinguishable from other remarks using regular expressions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#you must replace NA with the literal string &amp;quot;NA&amp;quot; so str_* functions from stringr can deal with it
input.all &amp;lt;- input.all %&amp;gt;% mutate(remark = str_replace_na(remark))

IDpattern &amp;lt;- &amp;quot;[:lower:][:blank:]\\d+[:blank:][:lower:]&amp;quot;
str_view(input.all$remark[1:10], IDpattern)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:960px;height:100%;&#34; class=&#34;str_view html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;ul&gt;\n  &lt;li&gt;08:30:38 Lamp: ParIn -&gt;  500 uml&lt;\/li&gt;\n  &lt;li&gt;08:32:37 CO2 Mixer: CO2R -&gt; 400 uml&lt;\/li&gt;\n  &lt;li&gt;08:40:20 Lamp: ParIn -&gt;  500 uml&lt;\/li&gt;\n  &lt;li&gt;08:40:20 CO2 Mixer: CO2R -&gt; 400 uml&lt;\/li&gt;\n  &lt;li&gt;08:40:20 Coolers: Off&lt;\/li&gt;\n  &lt;li&gt;08:40:20 Flow: Fixed -&gt; 500 umol/s&lt;\/li&gt;\n  &lt;li&gt;08:42:11 Flow: Fixed -&gt; 500 umol/s&lt;\/li&gt;\n  &lt;li&gt;08:43:13 &lt;span class=&#39;match&#39;&gt;c 4 a&lt;\/span&gt;&lt;\/li&gt;\n  &lt;li&gt;NA&lt;\/li&gt;\n  &lt;li&gt;NA&lt;\/li&gt;\n&lt;\/ul&gt;&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Now that I’ve figure out a pattern that matches the ID’s I can use &lt;code&gt;str_extract()&lt;/code&gt; to move them to a new &lt;code&gt;sampleID&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;input.all &amp;lt;- input.all %&amp;gt;%
  mutate(sampleID = str_extract(remark, IDpattern)) %&amp;gt;% 
  select(sampleID, everything())
head(input.all, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 42
##    sampleID remark Obs   HHMMSS FTime `EBal?` Photo    Cond    Ci Trmmol
##    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;time&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 &amp;lt;NA&amp;gt;     08:30… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA    
##  2 &amp;lt;NA&amp;gt;     08:32… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA    
##  3 &amp;lt;NA&amp;gt;     08:40… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA    
##  4 &amp;lt;NA&amp;gt;     08:40… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA    
##  5 &amp;lt;NA&amp;gt;     08:40… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA    
##  6 &amp;lt;NA&amp;gt;     08:40… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA    
##  7 &amp;lt;NA&amp;gt;     08:42… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA    
##  8 c 4 a    08:43… &amp;lt;NA&amp;gt;     NA     NA      NA NA    NA         NA NA    
##  9 &amp;lt;NA&amp;gt;     NA     1     08:43    932       0  6.62  0.0589   202  0.724
## 10 &amp;lt;NA&amp;gt;     NA     2     08:43    937       0  6.99  0.0594   193  0.731
## # … with 32 more variables: VpdL &amp;lt;dbl&amp;gt;, CTleaf &amp;lt;dbl&amp;gt;, Area &amp;lt;dbl&amp;gt;,
## #   BLC_1 &amp;lt;dbl&amp;gt;, StmRat &amp;lt;dbl&amp;gt;, BLCond &amp;lt;dbl&amp;gt;, Tair &amp;lt;dbl&amp;gt;, Tleaf &amp;lt;dbl&amp;gt;,
## #   TBlk &amp;lt;dbl&amp;gt;, CO2R &amp;lt;dbl&amp;gt;, CO2S &amp;lt;dbl&amp;gt;, H2OR &amp;lt;dbl&amp;gt;, H2OS &amp;lt;dbl&amp;gt;,
## #   RH_R &amp;lt;dbl&amp;gt;, RH_S &amp;lt;dbl&amp;gt;, Flow &amp;lt;dbl&amp;gt;, PARi &amp;lt;dbl&amp;gt;, PARo &amp;lt;dbl&amp;gt;,
## #   Press &amp;lt;dbl&amp;gt;, CsMch &amp;lt;dbl&amp;gt;, HsMch &amp;lt;dbl&amp;gt;, CsMchSD &amp;lt;dbl&amp;gt;, HsMchSD &amp;lt;dbl&amp;gt;,
## #   CrMchSD &amp;lt;dbl&amp;gt;, HrMchSD &amp;lt;dbl&amp;gt;, StableF &amp;lt;dbl&amp;gt;, BLCslope &amp;lt;dbl&amp;gt;,
## #   BLCoffst &amp;lt;dbl&amp;gt;, f_parin &amp;lt;dbl&amp;gt;, f_parout &amp;lt;dbl&amp;gt;, alphaK &amp;lt;dbl&amp;gt;,
## #   Status &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fill-down&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fill down&lt;/h2&gt;
&lt;p&gt;Now, if this were Excel, you could highlight that “c 4 a” and drag the corner down to fill in all the NA’s. In R, you can do exactly this with the &lt;code&gt;fill()&lt;/code&gt; function from &lt;code&gt;tidyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get rid of other remarks and fill down the sample ID column
output &amp;lt;- input.all %&amp;gt;% 
  filter(!xor(remark == &amp;quot;NA&amp;quot; , is.na(sampleID))) %&amp;gt;%
  fill(sampleID) %&amp;gt;% 
  #get rid of the rest of the remark rows
  filter(complete.cases(.)) %&amp;gt;% 
  #get rid of the remark column
  select(-remark)
head(output, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 41
##    sampleID Obs   HHMMSS FTime `EBal?` Photo   Cond    Ci Trmmol  VpdL
##    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;time&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 c 4 a    1     08:43    932       0  6.62 0.0589   202  0.724  1.25
##  2 c 4 a    2     08:43    937       0  6.99 0.0594   193  0.731  1.25
##  3 c 4 a    3     08:44    942       0  6.96 0.0596   195  0.731  1.25
##  4 c 4 b    4     08:44    978       0  7.08 0.0783   237  0.941  1.23
##  5 c 4 b    5     08:44    985       0  6.9  0.0791   242  0.949  1.23
##  6 c 4 b    6     08:44    990       0  6.78 0.0801   246  0.959  1.22
##  7 c 4 c    7     08:45   1030       0  6.34 0.0654   228  0.805  1.25
##  8 c 4 c    8     08:45   1037       0  6.48 0.0648   222  0.798  1.26
##  9 c 4 c    9     08:45   1044       0  6.51 0.0664   226  0.819  1.26
## 10 c 4 c    10    08:45   1050       0  6.68 0.0678   225  0.836  1.26
## # … with 31 more variables: CTleaf &amp;lt;dbl&amp;gt;, Area &amp;lt;dbl&amp;gt;, BLC_1 &amp;lt;dbl&amp;gt;,
## #   StmRat &amp;lt;dbl&amp;gt;, BLCond &amp;lt;dbl&amp;gt;, Tair &amp;lt;dbl&amp;gt;, Tleaf &amp;lt;dbl&amp;gt;, TBlk &amp;lt;dbl&amp;gt;,
## #   CO2R &amp;lt;dbl&amp;gt;, CO2S &amp;lt;dbl&amp;gt;, H2OR &amp;lt;dbl&amp;gt;, H2OS &amp;lt;dbl&amp;gt;, RH_R &amp;lt;dbl&amp;gt;,
## #   RH_S &amp;lt;dbl&amp;gt;, Flow &amp;lt;dbl&amp;gt;, PARi &amp;lt;dbl&amp;gt;, PARo &amp;lt;dbl&amp;gt;, Press &amp;lt;dbl&amp;gt;,
## #   CsMch &amp;lt;dbl&amp;gt;, HsMch &amp;lt;dbl&amp;gt;, CsMchSD &amp;lt;dbl&amp;gt;, HsMchSD &amp;lt;dbl&amp;gt;, CrMchSD &amp;lt;dbl&amp;gt;,
## #   HrMchSD &amp;lt;dbl&amp;gt;, StableF &amp;lt;dbl&amp;gt;, BLCslope &amp;lt;dbl&amp;gt;, BLCoffst &amp;lt;dbl&amp;gt;,
## #   f_parin &amp;lt;dbl&amp;gt;, f_parout &amp;lt;dbl&amp;gt;, alphaK &amp;lt;dbl&amp;gt;, Status &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, we have a cleaned data frame ready for use in analyses! You could go on to separate plot ID, plant ID and leaf ID using &lt;code&gt;separate()&lt;/code&gt; from &lt;code&gt;tidyr&lt;/code&gt;, and then do any necessary calculations, visualizations, and modeling with the resulting data frame.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Quantifying leafhopper damage with automated supervised classification</title>
      <link>http://ericrscott.com/2017/12/24/leafhopper-fiji/</link>
      <pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2017/12/24/leafhopper-fiji/</guid>
      <description>&lt;p&gt;As part of my fieldwork in China, I collected harvested tea leaves that were damaged by the tea green leafhopper. I want to quantify the amount of leafhopper damage for each harvest. I was able to find several solutions for quantifying holes in leaves or even damage to leaf margins, but typical leafhopper damage is just tiny brown spots on the undersides of leaves. I did find some tutorials on using &lt;a href=&#34;http://imagej.net/Welcome&#34;&gt;ImageJ&lt;/a&gt; to analyze diseased area on leaves, but found that the leafhopper damage spots were too small and too similar in color to undamaged leaves for these tools to work reliably and be automated.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span id=&#34;fig:leaf-fig&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/damaged leaf.png&#34; alt=&#34;Typical leafhopper damage&#34; width=&#34;60%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Typical leafhopper damage
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Last year I piloted a method to quantify leafhopper damage on scanned images of tea leaves with the help of a Tufts undergraduate, Maxwell Turpin. We ended up getting the most success using a supervised classification algorithm implemented by the &lt;a href=&#34;https://imagej.net/Trainable_Weka_Segmentation&#34;&gt;trainable WEKA segmentation&lt;/a&gt; plugin in &lt;a href=&#34;https://fiji.sc/&#34;&gt;FIJI&lt;/a&gt; (which stands for “FIJI is just Image J”). This semester, another Tufts undergraduate, Michelle Mu, worked on refining this approach, automating it, and applying it to the hundreds of images I obtained over the summer as part of my research.&lt;/p&gt;
&lt;div id=&#34;supervised-pixel-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Supervised pixel classification&lt;/h2&gt;
&lt;p&gt;Just to clarify, my goal here is not image classification—that is, I’m not trying to classify leaves into categories like “undamaged”, “medium damaged”, “high damage”, but rather trying to classify individual pixels in the image as being damaged or undamaged leaf tissue (or background).&lt;/p&gt;
&lt;p&gt;In short, after selecting some pixels representative of damaged leaf, undamaged leaf, and background (regions of interest, or ROIs), the WEKA plugin trains a random forest algorithm using data from various transformations of the pixels in the ROIs. Then, I can apply the algorithm to other images and extract data in the form of numbers of pixels classified in each category.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:example-result&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/analyzed leaf.png&#34; alt=&#34;example results of WEKA classification&#34; width=&#34;60%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: example results of WEKA classification
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;weka-segmentation-tips&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;WEKA segmentation tips&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://imagej.net/Trainable_Weka_Segmentation&#34;&gt;documentation&lt;/a&gt; on the WEKA segmentation plugin is fairly detailed, so I won’t go into great detail on how to use it, rather focus on some things I learned specific to this project.&lt;/p&gt;
&lt;div id=&#34;creating-a-training-stack&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating a training stack&lt;/h3&gt;
&lt;p&gt;I had hundreds of images to classify, so obviously it made sense to train a classifier on a subset of leaves. We started by taking my leaf scans, which contained dozens of leaves, and making images of individual leaves. You can do this in any number of image manipulation software, but we found it easiest using Preview, the default image and PDF viewer on OS X. You just select a leaf with the rectangle selection tool, copy with cmd + c, and create a new image with cmd + n, then save with cmd + s.&lt;/p&gt;
&lt;p&gt;We then chose a random subset of 15 leaves to use as a training set. Why 15? At the time, we were using a regular desktop computer with 8 GB of RAM, and using the WEKA plugin with an image stack any larger than that caused it to crash. Fortunately, because the leaves were all scanned in a uniform way and leaf color didn’t vary too much, 15 leaves was suitable for a training set. If you have more RAM at your disposal, feel free to train on more leaves.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-and-applying-a-classifier&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Training and applying a classifier&lt;/h3&gt;
&lt;p&gt;We trained the classifier on three classes, background, damaged leaf, and undamaged leaf. For background and undamaged leaf, we found it was really important to focus on leaf edges, making sure to include shadows in the background class and lighter green leaf margins in the undamaged class. Without doing this, the classifier would consistently mis-classify shadows and edges as either damaged or background, respectively. This was also an iterative process and took several rounds of selecting ROIs, training a classifier, viewing results, and adding more ROIs. Once we were satisfied with our classifier, we saved it and applied it to all of our images in stacks of 20 on a computer with 32 GB of RAM. The results
created by the WEKA plugin are stacks of three color images (because we used 3 classes). Getting numerical results turned out to be another problem.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:screenshot&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;http://ericrscott.com/img/WEKA screenshot.png&#34; alt=&#34;WEKA segmentation window with ROIs selected&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: WEKA segmentation window with ROIs selected
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exporting results&lt;/h3&gt;
&lt;p&gt;Exporting results in a numeric format turned out to be a lot more difficult than we thought it would. The manual way of doing this through the FIJI menus is &lt;em&gt;Analyze &amp;gt; Histogram&lt;/em&gt; which opens a histogram window, then clicking “list” to get a results window with the number of pixels in each class for that image, then copying and pasting into Excel. This was far too labor intensive and error-prone to be appropriate for hundreds of images. We needed a better way, which led us to FIJI macros.&lt;/p&gt;
&lt;p&gt;Building a macro turned out to be relatively painless, even though neither Michelle nor I had any experience coding in any language other than R. Through a combination of forum posts and using the documentation for the &lt;a href=&#34;http://imagej.net/developer/macro/functions.html&#34;&gt;ImageJ macro language&lt;/a&gt; as a reference, we were able to create a macro that opens results stacks (three-color images) and exports a text file containing the number of pixels in each class for each image in the stack.&lt;/p&gt;
&lt;pre class=&#34;javascript&#34;&gt;&lt;code&gt;//ImageJ macro for exporting numerical results from classified image stacks
inpath = getDirectory(&amp;quot;Analyzed Stacks&amp;quot;);
File.makeDirectory(inpath + &amp;quot;//Results//&amp;quot;);
//outpath = getDirectory(&amp;quot;Results&amp;quot;);
//for some reason using getDirectory() twice screws things up.
//My solution is to just create a results folder in the folder with the analyzed stacks
//But then you get an error at the end when the script tries to open that folder.
files = getFileList(inpath);
for(j = 0; j &amp;lt; lengthOf(files); j++){
    open(files[j]);
    title = getTitle();
    for (n = 1; n &amp;lt;= nSlices(); n++) { //loop through slices
        showProgress(n, nSlices); //this just adds a progress bar
        setSlice(n); //set which slice
        getStatistics(area, mean, min, max, std, histogram); //this gets the number of pixels
        for (i=0; i&amp;lt;histogram.length; i++) {
            setResult(&amp;quot;Value&amp;quot;, i, i);
            setResult(&amp;quot;Leaf.&amp;quot; + n, i, histogram[i]); //adds a column for each slice called &amp;quot;Count[slicenumber]&amp;quot;
        }   
    saveAs(&amp;quot;results&amp;quot;, inpath + &amp;quot;//Results//&amp;quot; + title + &amp;quot;.txt&amp;quot;); //saves results table as text file
    }
    close();
    run(&amp;quot;Clear Results&amp;quot;);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;importing-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Importing Results&lt;/h3&gt;
&lt;p&gt;The results files then get read into R and tidied using a relatively simple script.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#packages you will need
library(readr)
library(dplyr)
library(tidyr)
library(stringr)

#Get the filenames of all the results files
filenames &amp;lt;- list.files(&amp;quot;Results/&amp;quot;) #might need to change path

#Create paths to those files
filepaths &amp;lt;- paste0(&amp;quot;Results/&amp;quot;, filenames)

#make a list to eventually contain all the data files.  
raw.list &amp;lt;- as.list(filenames)
names(raw.list) &amp;lt;- filenames
raw.list

#for loop for reading in every file into an element of the list.  There is probably a faster way to do this with purrr::map
for(i in 1:length(filenames)){   #loop through all files
  raw.list[[i]] &amp;lt;- read_tsv(filepaths[i])[1:2, -1] #read only the first two rows and NOT the first column into the list
}
raw.list #now contains multiple data frames.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, we have created a list of data frames, one for each image stack. Since our image stacks were split up just to make analysis possible with limited RAM, we want to merge the results back together now.&lt;/p&gt;
&lt;p&gt;I also couldn’t figure out how to rename the classes in the Image J macro (they appear as numeric labels “0”, “1”, and “2”), so we took this opportunity to rename them in our R script.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raw.data &amp;lt;- raw.list %&amp;gt;%
  bind_rows(.id = &amp;quot;File&amp;quot;) %&amp;gt;% 
  mutate(Value = ifelse(Value == 0, &amp;quot;damaged&amp;quot;, &amp;quot;undamaged&amp;quot;) %&amp;gt;% as.factor()) %&amp;gt;% 
  #converts 0&amp;#39;s to &amp;quot;damaged&amp;quot; and anything else to &amp;quot;undamaged&amp;quot;, then converts Value to a factor
  rename(Type = &amp;quot;Value&amp;quot;)
  #renames the &amp;quot;Value&amp;quot; column &amp;quot;Type&amp;quot;

raw.data.2 &amp;lt;- raw.data %&amp;gt;% 
  gather(-File, -Type, key = LeafID, value = Pixels) %&amp;gt;%  #gathers all the data into three columns
  spread(key = Type, value = Pixels)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finished! With all the image stacks analyzed using our classifier, numeric results exported using our custom macro, and then read into R and tidied using our R script, we have data ready for statistical analysis!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Making a website in RStudio</title>
      <link>http://ericrscott.com/2017/12/19/making-a-website-in-rstudio/</link>
      <pubDate>Tue, 19 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>http://ericrscott.com/2017/12/19/making-a-website-in-rstudio/</guid>
      <description>&lt;p&gt;I’ve always thought I should eventually have a professional website for job hunting, but recently I’ve realized that it would be nice to have somewhere to collect my thoughts and contributions all in one place.
Before having a website of my own, I generally shared updates and thoughts on &lt;a href=&#34;https://twitter.com/LeafyEricScott&#34;&gt;Twitter&lt;/a&gt; and on &lt;a href=&#34;#blogs&#34;&gt;other people’s blogs&lt;/a&gt;, which I will still continue to do, but having everything I do in one place seems like a good idea. I kept putting off making a website until I was recently encouraged by a combination of envy of my &lt;a href=&#34;https://www.rachaelebonoan.com/&#34;&gt;officemate’s awesome looking website&lt;/a&gt;, and learning that one could make a website using R Studio and the &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown&lt;/a&gt; R package. I just spent a semester learning a ton about R Markdown, the file type blogdown uses to construct websites, for a &lt;a href=&#34;https://github.com/Aariq/biostats-recitation&#34;&gt;biostatistics course&lt;/a&gt; I was teaching, so I figured using R Markdown to make a website would be relatively easy. Fortunately, I was correct! I spent about 3-4 work days initially getting this website working and customized, and it was relatively painless. Compared to, say WordPress, the impressiveness-to-effort ratio is much higher, in my opinion.&lt;/p&gt;
&lt;div id=&#34;getting-started-with-blogdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting started with Blogdown&lt;/h2&gt;
&lt;p&gt;Basically blogdown builds a website from a series of Markdown or R Markdown documents. Markdown and the more R-specific version, R Markdown, are really simple languages for creating formatted HTML documents. For example, you can create headers like the one at the start of this paragraph simply by adding different numbers of “#”, and turn a word into a link with square brackets followed by a URL. When the Markdown is converted to HTML, those special characters get converted into the relevant HTML code. Blogdown adds some features to R Studio that make it easier to import themes to help you get started, write new blog posts, and view changes to your website “live.”&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://bookdown.org/yihui/blogdown/get-started.html&#34;&gt;tutorial for blogdown&lt;/a&gt; is great, but I found that choosing a &lt;a href=&#34;https://themes.gohugo.io/&#34;&gt;theme&lt;/a&gt; early on was important, because a lot of what I needed to learn and figure out was specific to the theme. In fact, using blogdown wasn’t really as helpful as I thought for editing the theme I chose, as the example pages in the theme were written in a different format (Markdown with TOML headers) than the default for a new blogdown post (R Markdown with YAML header). That being said, the example Markdown documents that came with the theme were well annotated and for the most part all I had to do was fiddle around with values and add my own text and images.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-and-hosting-a-website&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building and Hosting a website&lt;/h2&gt;
&lt;p&gt;I decided to go with blogdown’s recommendation to build and host my website using &lt;a href=&#34;https://www.netlify.com/&#34;&gt;Netlify&lt;/a&gt;. The tutorials on Netlify are very jargon-y and I got basically nothing out of them. Fortunately, the &lt;a href=&#34;https://bookdown.org/yihui/blogdown/netlify.html&#34;&gt;chapter on Netlify&lt;/a&gt; in the blogdown book was very helpful. Netlify builds your website from a GitHub repository containing all the Markdown or R Markdown files you’ve created. Fortunately, I had just recently figured out (after &lt;em&gt;much&lt;/em&gt; failure) how to link GitHub to R Studio thanks to &lt;a href=&#34;http://happygitwithr.com/&#34;&gt;Jenny Bryan&lt;/a&gt;, so it was just a matter of pushing updates to my website to GitHub and following the blogdown tutorial to build the website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;want-to-make-a-website-just-try-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Want to make a website? Just try it!&lt;/h2&gt;
&lt;p&gt;All of this was completely free, and if you have &lt;em&gt;some&lt;/em&gt; knowledge of R Markdown it is super easy to get started. Even if you’ve never used R Markdown, it’s not hard to learn. Depending on the theme you choose, you could get a website up and running in a day or two easily. Feel free to check out the &lt;a href=&#34;https://github.com/Aariq/my-website&#34;&gt;source documents&lt;/a&gt; for this website. I think you’ll find that they are not as intimidating as you might think!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
